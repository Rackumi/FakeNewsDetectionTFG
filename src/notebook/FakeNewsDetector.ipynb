{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAKE NEWS DETECTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importación de librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas son las librerias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalMaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de datos y preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos todo el dataset: https://www.kaggle.com/competitions/fake-news/data\n",
    "\n",
    "El tran.csv contendrá los datos para entrenar.\n",
    "\n",
    "El test.csv y submit.csv son los datos para el testeo y sus respuestas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv (r'C:\\Users\\alvar\\Desktop\\TFG\\Dataset\\dataset1\\train.csv').fillna('')\n",
    "test = pd.read_csv (r'C:\\Users\\alvar\\Desktop\\TFG\\Dataset\\dataset1\\test.csv').fillna('')\n",
    "submit = pd.read_csv (r'C:\\Users\\alvar\\Desktop\\TFG\\Dataset\\dataset1\\submit.csv').fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rellenamos los posibles huecos nulos con \"fillna()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos del train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>3890</td>\n",
       "      <td>Why Are Basketball Games So Squeaky? Consider ...</td>\n",
       "      <td>John Branch</td>\n",
       "      <td>It is the unofficial soundtrack of basketball,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11229</th>\n",
       "      <td>11229</td>\n",
       "      <td>Giant James May terrorises Kent after being gi...</td>\n",
       "      <td>Guest</td>\n",
       "      <td>November 17, 2016 \\nKent woke today to scenes ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6274</th>\n",
       "      <td>6274</td>\n",
       "      <td>Obama Not Going To Bother Fixing White House T...</td>\n",
       "      <td>Gerry McBride</td>\n",
       "      <td>0 Add Comment \\n“IT’S not like I’m getting my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "3890    3890  Why Are Basketball Games So Squeaky? Consider ...   \n",
       "11229  11229  Giant James May terrorises Kent after being gi...   \n",
       "6274    6274  Obama Not Going To Bother Fixing White House T...   \n",
       "\n",
       "              author                                               text  label  \n",
       "3890     John Branch  It is the unofficial soundtrack of basketball,...      0  \n",
       "11229          Guest  November 17, 2016 \\nKent woke today to scenes ...      1  \n",
       "6274   Gerry McBride  0 Add Comment \\n“IT’S not like I’m getting my ...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos del test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>23814</td>\n",
       "      <td>Watch: Hillary Clinton Can’t Stop Coughing Dur...</td>\n",
       "      <td>Pam Key</td>\n",
       "      <td>Friday during her commencement speech at Welle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>23869</td>\n",
       "      <td>Donald Trump: I Would Let My Son Barron Play F...</td>\n",
       "      <td>Charlie Spiering</td>\n",
       "      <td>President Donald Trump would be willing to let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>21825</td>\n",
       "      <td>Donald Trump Probably Stanched His Losses, but...</td>\n",
       "      <td>Nate Cohn</td>\n",
       "      <td>The bar was low for Donald J. Trump after the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "3014  23814  Watch: Hillary Clinton Can’t Stop Coughing Dur...   \n",
       "3069  23869  Donald Trump: I Would Let My Son Barron Play F...   \n",
       "1025  21825  Donald Trump Probably Stanched His Losses, but...   \n",
       "\n",
       "                author                                               text  \n",
       "3014           Pam Key  Friday during her commencement speech at Welle...  \n",
       "3069  Charlie Spiering  President Donald Trump would be willing to let...  \n",
       "1025         Nate Cohn  The bar was low for Donald J. Trump after the ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    0 - True\n",
    "    1 - False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>21081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>24147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>20867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  label\n",
       "281   21081      1\n",
       "3347  24147      0\n",
       "67    20867      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Concatenar autor y título"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui juntamos el autor y el titulo en una misma columna para analizarlos juntos. También eliminamos las columnas que ya no nos sirven como text y author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.title = train.title + train.author\n",
    "test.title = test.title + test.author\n",
    "train.drop(columns=['text', 'author'], inplace=True)\n",
    "test.drop(columns=['text', 'author'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Stopwords son palabras comunes que se filtran o eliminan en el procesamiento del lenguaje natural, ya que se consideran irrelevantes para el análisis de texto o la búsqueda de información. Estas palabras incluyen pronombres, preposiciones, conjunciones y otras palabras que se usan con frecuencia en el lenguaje cotidiano, pero que no aportan información significativa sobre el tema o contenido de un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = {\" \", \"a\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \n",
    "             \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \n",
    "             \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \n",
    "             \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \n",
    "             \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \n",
    "             \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \n",
    "             \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \n",
    "             \"ax\", \"ay\", \"az\", \"b\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \n",
    "             \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \n",
    "             \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \n",
    "             \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \n",
    "             \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \n",
    "             \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \n",
    "             \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \n",
    "             \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \n",
    "             \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \n",
    "             \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \n",
    "             \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \n",
    "             \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \n",
    "             \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \n",
    "             \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \n",
    "             \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \n",
    "             \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \n",
    "             \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \n",
    "             \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \n",
    "             \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \n",
    "             \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \n",
    "             \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \n",
    "             \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \n",
    "             \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \n",
    "             \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \n",
    "             \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \n",
    "             \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \n",
    "             \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \n",
    "             \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \n",
    "             \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \n",
    "             \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \n",
    "             \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \n",
    "             \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \n",
    "             \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \n",
    "             \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \n",
    "             \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \n",
    "             \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \n",
    "             \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \n",
    "             \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \n",
    "             \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \n",
    "             \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \n",
    "             \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \n",
    "             \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \n",
    "             \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \n",
    "             \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \n",
    "             \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \n",
    "             \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \n",
    "             \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \n",
    "             \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \n",
    "             \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \n",
    "             \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \n",
    "             \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \n",
    "             \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \n",
    "             \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \n",
    "             \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \n",
    "             \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \n",
    "             \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \n",
    "             \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \n",
    "             \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \n",
    "             \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \n",
    "             \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \n",
    "             \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\",\n",
    "             \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \n",
    "             \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \n",
    "             \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \n",
    "             \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \n",
    "             \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \n",
    "             \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \n",
    "             \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \n",
    "             \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \n",
    "             \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \n",
    "             \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \n",
    "             \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \n",
    "             \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \n",
    "             \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \n",
    "             \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \n",
    "             \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \n",
    "             \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \n",
    "             \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \n",
    "             \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \n",
    "             \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Funcion filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a procesar los datos de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definimos el Stemming:\n",
    "    \n",
    "Stemming es el proceso mediante el cual se reduce una palabra a su raiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitaremos la funcion filtro:\n",
    "\n",
    "Quitaremos la raiz de las palabras, stopWords, reduciremos a palabras sin caracteres extraños, todo en minúscula. Entre otras cosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtro(content):\n",
    "    contenido = re.sub('[^a-zA-Z]',' ',content)\n",
    "    contenido = contenido.lower()\n",
    "    contenido = contenido.split()\n",
    "    contenido = [ps.stem(word) for word in contenido if word not in stopWords]\n",
    "    contenido = list(map(ps.stem, contenido))\n",
    "    contenido = ' '.join(contenido)\n",
    "    return contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Procesamiento con el filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtramos tanto los datos de entrenamiento como los de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.title = train.title.apply(filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hou dem aid comey letter jason chaffetz tweet ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>flynn hillari clinton big woman campu breitbar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truth firedconsortiumnew</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>civilian kill singl airstrik identifiedjessica...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>iranian woman jail fiction unpublish stori wom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jacki mason hollywood love trump bomb north ko...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>life life luxuri elton john favorit shark pict...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>beno hamon win french socialist parti presiden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>excerpt draft script donald trump ampa black c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>channel plan ukrain russia courtesi trump asso...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  label\n",
       "0   0  hou dem aid comey letter jason chaffetz tweet ...      1\n",
       "1   1  flynn hillari clinton big woman campu breitbar...      0\n",
       "2   2                           truth firedconsortiumnew      1\n",
       "3   3  civilian kill singl airstrik identifiedjessica...      1\n",
       "4   4  iranian woman jail fiction unpublish stori wom...      1\n",
       "5   5  jacki mason hollywood love trump bomb north ko...      0\n",
       "6   6  life life luxuri elton john favorit shark pict...      1\n",
       "7   7  beno hamon win french socialist parti presiden...      0\n",
       "8   8  excerpt draft script donald trump ampa black c...      0\n",
       "9   9  channel plan ukrain russia courtesi trump asso...      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.title = test.title.apply(filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>specter trump loosen tongu pur string silicon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>russian warship readi strike terrorist aleppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>nodapl nativ american leader vow stay winter f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>tim tebow attempt comeback time baseb york tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>keiser report meme war truth broadcast network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20805</td>\n",
       "      <td>trump usa antiqu hero clinton presid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20806</td>\n",
       "      <td>pelosi call fbi investig russian donald trump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20807</td>\n",
       "      <td>weekli featur profil randi shannontrevor loudon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20808</td>\n",
       "      <td>urban popul boom climat chang wor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20809</td>\n",
       "      <td>cognit dissid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title\n",
       "0  20800  specter trump loosen tongu pur string silicon ...\n",
       "1  20801      russian warship readi strike terrorist aleppo\n",
       "2  20802  nodapl nativ american leader vow stay winter f...\n",
       "3  20803  tim tebow attempt comeback time baseb york tim...\n",
       "4  20804     keiser report meme war truth broadcast network\n",
       "5  20805               trump usa antiqu hero clinton presid\n",
       "6  20806  pelosi call fbi investig russian donald trump ...\n",
       "7  20807    weekli featur profil randi shannontrevor loudon\n",
       "8  20808                  urban popul boom climat chang wor\n",
       "9  20809                                      cognit dissid"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Vectorizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La vectorización con TF-IDF es un proceso que convierte el texto en un conjunto de números, que pueden ser utilizados para analizar o buscar información en el texto. Esta técnica utiliza una fórmula para asignar valores numéricos a cada palabra en un documento, basándose en la frecuencia con que aparece en el documento y en el corpus completo de documentos. Los documentos que contienen palabras similares tendrán vectores numéricos similares, lo que facilita la comparación y el análisis de documentos mediante técnicas de aprendizaje automático y minería de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVec = vectorizer.fit_transform(train.title.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testVec = vectorizer.transform(test.title.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800, 23135)\n",
      "(5200, 23135)\n"
     ]
    }
   ],
   "source": [
    "print(trainVec.shape)\n",
    "print(testVec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a bajar el número de componentes de los 23050 hasta una dimensionalidad de 10.\n",
    "\n",
    "10 es un hiperparametro de dimensionalidad muy competitivo para este problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionalidad = 5\n",
    "tsvd = TruncatedSVD(n_components=dimensionalidad)\n",
    "\n",
    "trainVec = tsvd.fit_transform(trainVec)\n",
    "testVec = tsvd.transform(testVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800, 5)\n",
      "(5200, 5)\n"
     ]
    }
   ],
   "source": [
    "print(trainVec.shape)\n",
    "print(testVec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar la dimesionalidad del train y del test han sido reducidas hasta 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación y entrenamiento.\n",
    "\n",
    "Dividiremos el fichero train en dos subconjuntos. Uno del 80% y otro del 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(trainVec, train.label, test_size=0.2, stratify=train.label, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se van a usar cuatro modelos distintos para realizar este problema. Mas tarde se compararán sus resultados.\n",
    "\n",
    "- Modelo SVC no lineal\n",
    "- Modelo no lineal red neuronal simple\n",
    "- Modelo no lineal red neuronal compleja (Deep learning)\n",
    "- Modelo no lineal red neuronal transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada modelo tenemos que obtener los hiperparámetros óptimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos una función para hacer que los resultados de las redes neuronales sean binarios ya que necesitamos clasificar entre verdaderos y falsos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizador(p, pAux, x):\n",
    "    for i in p:\n",
    "        if i >= x:\n",
    "            pAux.append(1)\n",
    "        else: \n",
    "            pAux.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Modelo SVC no lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC: El \"Support Vector Classifier\" es un clasificador binario. Este funciona con un algoritmo de aprendizaje automático supervisado utilizado para clasificar datos en dos o más categorías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Selección de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los hiperparametros que tendremos que tener en cuenta serán el kernel del algoritmo el cual lo dejaremos en polinomico con su degree a 4. Con el parámetro de regularización a 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.5, degree=4, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=0.5, degree=4, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=0.5, degree=4, kernel='poly')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC = svm.SVC(kernel=\"poly\", C=0.5, degree=4)\n",
    "SVC.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction1 = SVC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8079326923076923\n",
      "Precision:  0.9942390782525204\n",
      "Recall:  0.7246326102169349\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score(train_prediction1, Y_test))\n",
    "print(\"Precision: \",precision_score(train_prediction1, Y_test))\n",
    "print(\"Recall: \",recall_score(train_prediction1, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Modelo no lineal: Red neuronal simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red neuronal simple: Una red neuronal simple puede utilizarse para clasificar noticias como verdaderas o falsas basándose en características específicas. Sin embargo, es probable que no tenga la capacidad suficiente para identificar patrones muy complejos y sutiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Selección de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos solo una capa ya que es una red neuronal simple.\n",
    "\n",
    "En este caso los hiperparámetros son el tipo de conexion entre la capa única y la de salida, el número de neuronas en la capa única y el número de épocas.\n",
    "\n",
    "Los hiperparametros seleccionados serán los siguientes:\n",
    "\n",
    "\n",
    "- Unidades en la capa densa (5 y 1): Esta es la cantidad de neuronas en la capa oculta y la capa de salida, respectivamente. En este caso, la capa oculta tiene 5 neuronas y la capa de salida tiene 1 neurona.\n",
    "\n",
    "\n",
    "- Función de activación ('sigmoid'): Esta es la función de activación utilizada en la capa de salida. La función de activación 'sigmoid' convierte los valores de entrada a un rango entre 0 y 1, lo que la hace adecuada para problemas de clasificación binaria.\n",
    "\n",
    "\n",
    "- Optimizador ('Adam') y tasa de aprendizaje (0.1): El optimizador es el algoritmo utilizado para ajustar los pesos de la red durante el entrenamiento con el fin de minimizar la función de pérdida. La tasa de aprendizaje es un hiperparámetro que determina cuánto cambian los pesos del modelo en respuesta a los datos de entrenamiento. En este caso, se utiliza una tasa de aprendizaje de 0.1, que es relativamente alta.\n",
    "\n",
    "\n",
    "- Función de pérdida ('binary_crossentropy'): La función de pérdida mide qué tan bien el modelo está realizando su tarea. Para un problema de clasificación binaria como este, se suele utilizar la entropía cruzada binaria.\n",
    "\n",
    "\n",
    "- Métrica ('accuracy'): Las métricas son utilizadas para evaluar el rendimiento del modelo. En este caso, se utiliza la precisión, que mide la proporción de predicciones correctas.\n",
    "\n",
    "\n",
    "- Número de épocas (10): Las épocas son el número de veces que el algoritmo de aprendizaje funcionará en todo el conjunto de datos de entrenamiento. En este caso, se utilizarán 10 épocas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "capaUnica = tf.keras.layers.Dense(units=5)\n",
    "salidaRNS = tf.keras.layers.Dense(units=1, activation = 'sigmoid')\n",
    "RNS = tf.keras.Sequential([capaUnica, salidaRNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNS.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(0.1),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20483a67850>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNS.fit(X_train, Y_train, epochs=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "train_prediction2 = RNS.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction2Aux = []\n",
    "binarizador(train_prediction2, train_prediction2Aux, 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.803125\n",
      "Precision:  0.9054248679788767\n",
      "Recall:  0.751993620414673\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score(train_prediction2Aux, Y_test))\n",
    "print(\"Precision: \",precision_score(train_prediction2Aux, Y_test))\n",
    "print(\"Recall: \",recall_score(train_prediction2Aux, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Modelo no lineal: Red neuronal compleja (Deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red neuronal compleja (Deep learning): Una red neuronal compleja, también conocida como deep learning, es una evolución de las redes neuronales simples. Puede procesar una gran cantidad de datos y aprender características con cierta complejidad y sutileza que pueden indicar una noticia falsa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Selección de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos varias capas ya que es una red neuronal compleja también conocido como Deep Learning.\n",
    "\n",
    "En este caso los hiperparámetros son el número de capas, el tipo de conexion entre ellas, el número de neuronas en cada capa y el número de épocas.\n",
    "\n",
    "Los hiperparametros seleccionados serán los siguientes:\n",
    "\n",
    "\n",
    "- Unidades en las capas densas (1, 2, 1, 2, 1): Estos son los números de neuronas en cada una de las capas de la red neuronal. En este caso, las capas ocultas tienen 1, 2, 1, y 2 neuronas, respectivamente, y la capa de salida tiene 1 neurona.\n",
    "\n",
    "\n",
    "- Optimizador ('Adam') y tasa de aprendizaje (0.1): El optimizador es el algoritmo que se utiliza para ajustar los pesos de la red durante el entrenamiento para minimizar la función de pérdida. En este caso se utiliza el optimizador Adam con una tasa de aprendizaje de 0.1. La tasa de aprendizaje es un hiperparámetro que determina cuánto se modifican los pesos del modelo en respuesta a los datos de entrenamiento.\n",
    "\n",
    "\n",
    "- Función de pérdida ('mean_squared_error'): La función de pérdida es lo que el modelo intenta minimizar durante el entrenamiento. En este caso se usa el error cuadrático medio (MSE).\n",
    "\n",
    "\n",
    "- Métrica ('accuracy'): Las métricas son usadas para evaluar el rendimiento del modelo. En este caso, se está utilizando la precisión (accuracy), que mide la proporción de predicciones correctas. \n",
    "\n",
    "\n",
    "- Número de épocas (20): Las épocas son el número de veces que el algoritmo de aprendizaje trabajará en todo el conjunto de datos de entrenamiento. En este caso, se utilizarán 20 épocas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "capaOc1 = tf.keras.layers.Dense(units=1)\n",
    "capaOc2 = tf.keras.layers.Dense(units=2)\n",
    "capaOc3 = tf.keras.layers.Dense(units=1)\n",
    "capaOc4 = tf.keras.layers.Dense(units=2)\n",
    "salidaRNC = tf.keras.layers.Dense(units=1)\n",
    "RNC = tf.keras.Sequential([capaOc1, capaOc2, capaOc3, capaOc4, salidaRNC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNC.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(0.1),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x204837da460>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNC.fit(X_train, Y_train, epochs=20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "train_prediction3 = RNC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction3Aux = []\n",
    "binarizador(train_prediction3, train_prediction3Aux, 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7978365384615385\n",
      "Precision:  0.9884781565050408\n",
      "Recall:  0.7159248956884562\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score(train_prediction3Aux, Y_test))\n",
    "print(\"Precision: \",precision_score(train_prediction3Aux, Y_test))\n",
    "print(\"Recall: \",recall_score(train_prediction3Aux, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Modelo no lineal: Red neuronal transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red neuronal transformer: Una red neuronal transformer puede procesar secuencias de texto y aprender patrones en el lenguaje que puedan indicar una noticia falsa. Puede ser entrenada para identificar el lenguaje engañoso y las tácticas comunes utilizadas en las noticias falsas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Selección de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los hiperparametros utilizados serán los siguientes:\n",
    "\n",
    "\n",
    "- Tamaño de entrada (input_shape): Esta es la dimensionalidad de los datos de entrada. Es un hiperparámetro que debe ser igual a la cantidad de características en los datos de entrada.\n",
    "\n",
    "\n",
    "- Unidades en las capas densas (256, 128, 64, 32, 16, 1): Este es el número de neuronas en cada capa. Es un hiperparámetro que puedes ajustar para mejorar el rendimiento del modelo. Generalmente, un mayor número de neuronas puede capturar más complejidad, pero también puede causar sobreajuste.\n",
    "\n",
    "\n",
    "- Funciones de activación ('relu', 'sigmoid'): Las funciones de activación determinan la salida de una neurona dada su entrada. 'Relu' (Rectified Linear Unit) es una función de activación común en las capas ocultas, mientras que 'Sigmoid' se utiliza a menudo en la capa de salida para tareas de clasificación binaria.\n",
    "\n",
    "\n",
    "- Tasa de dropout (0.2): El Dropout es una técnica de regularización que ayuda a prevenir el sobreajuste. Durante el entrenamiento, algunas neuronas de la red (en este caso, el 20% en cada capa) son ignoradas o \"apagadas\" al azar. Esto obliga a la red a aprender representaciones más robustas de los datos.\n",
    "\n",
    "\n",
    "- Número de capas (5 capas ocultas): Este es el número de capas en la red, excluyendo las capas de entrada y salida. Aumentar el número de capas puede hacer que el modelo sea más profundo y capaz de aprender representaciones más complejas, pero también puede llevar a sobreajuste y requiere más datos de entrenamiento.\n",
    "\n",
    "\n",
    "- Tipo de modelo (Dense, Dropout, Input, Model): Este es el tipo de cada capa en la red. 'Dense' es una capa completamente conectada, 'Dropout' es una capa de regularización, 'Input' es la capa de entrada, y 'Model' es la capa final que combina todo en un modelo Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(dimensionalidad))\n",
    "x = Dense(256, activation='relu')(input_layer)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "RNT = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNT.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20484035ee0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNT.fit(X_train, Y_train, batch_size=32, epochs=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "train_prediction4 = RNT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction4Aux = []\n",
    "binarizador(train_prediction4, train_prediction4Aux, 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8651442307692307\n",
      "Precision:  0.9380700912145943\n",
      "Recall:  0.8189438390611903\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \",accuracy_score(train_prediction4Aux, Y_test))\n",
    "print(\"Precision: \",precision_score(train_prediction4Aux, Y_test))\n",
    "print(\"Recall: \",recall_score(train_prediction4Aux, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a probar nuestro modelos entrenados en el dataset de test. Esto nos dará un porcentaje de acierto en base a como de eficaces sean los modelos. También se harán comparaciones entre modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Modelo Lineal SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction1 = SVC.predict(testVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metricas para evaluar modelo SVC: \n",
      "\n",
      "Accuracy:  0.7507692307692307\n",
      "Precision:  0.8902481649772807\n",
      "Recall:  0.7217342023236044\n",
      "F1 Score:  0.7971830985915493\n"
     ]
    }
   ],
   "source": [
    "print(\"Metricas para evaluar modelo SVC: \\n\")\n",
    "print(\"Accuracy: \",accuracy_score(test_prediction1, submit['label'].values))\n",
    "print(\"Precision: \",precision_score(test_prediction1, submit['label'].values))\n",
    "print(\"Recall: \",recall_score(test_prediction1, submit['label'].values))\n",
    "print(\"F1 Score: \",f1_score(test_prediction1, submit['label'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modelo no lineal: Red neuronal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction2 = RNS.predict(testVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction2Aux = []\n",
    "binarizador(test_prediction2, test_prediction2Aux, 0.2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metricas para evaluar modelo RNS: \n",
      "\n",
      "Accuracy:  0.759423076923077\n",
      "Precision:  0.9084236281020622\n",
      "Recall:  0.7243589743589743\n",
      "F1 Score:  0.806016436656846\n"
     ]
    }
   ],
   "source": [
    "print(\"Metricas para evaluar modelo RNS: \\n\")\n",
    "print(\"Accuracy: \",accuracy_score(test_prediction2Aux, submit['label'].values))\n",
    "print(\"Precision: \",precision_score(test_prediction2Aux, submit['label'].values))\n",
    "print(\"Recall: \",recall_score(test_prediction2Aux, submit['label'].values))\n",
    "print(\"F1 Score: \",f1_score(test_prediction2Aux, submit['label'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Modelo no lineal: Red neuronal compleja (Deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction3 = RNC.predict(testVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction3Aux = []\n",
    "binarizador(test_prediction3, test_prediction3Aux, 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metricas para evaluar modelo RNC: \n",
      "\n",
      "Accuracy:  0.7596153846153846\n",
      "Precision:  0.9108703250611674\n",
      "Recall:  0.7236878644820883\n",
      "F1 Score:  0.8065614360878984\n"
     ]
    }
   ],
   "source": [
    "print(\"Metricas para evaluar modelo RNC: \\n\")\n",
    "print(\"Accuracy: \",accuracy_score(test_prediction3Aux, submit['label'].values))\n",
    "print(\"Precision: \",precision_score(test_prediction3Aux, submit['label'].values))\n",
    "print(\"Recall: \",recall_score(test_prediction3Aux, submit['label'].values))\n",
    "print(\"F1 Score: \",f1_score(test_prediction3Aux, submit['label'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Modelo no lineal: Red neuronal transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction4 = RNT.predict(testVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction4Aux = []\n",
    "binarizador(test_prediction4, test_prediction4Aux, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metricas para evaluar modelo RNT: \n",
      "\n",
      "Accuracy:  0.7623076923076924\n",
      "Precision:  0.9150646627053478\n",
      "Recall:  0.7250069232899474\n",
      "F1 Score:  0.8090234857849197\n"
     ]
    }
   ],
   "source": [
    "print(\"Metricas para evaluar modelo RNT: \\n\")\n",
    "print(\"Accuracy: \",accuracy_score(test_prediction4Aux, submit['label'].values))\n",
    "print(\"Precision: \",precision_score(test_prediction4Aux, submit['label'].values))\n",
    "print(\"Recall: \",recall_score(test_prediction4Aux, submit['label'].values))\n",
    "print(\"F1 Score: \",f1_score(test_prediction4Aux, submit['label'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Comparación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver la precisión de los modelos van mejorando ligeramente a medida que son mas complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC:  0.7507692307692307\n",
      "RNS:  0.759423076923077\n",
      "RNC:  0.7596153846153846\n",
      "RNT:  0.7623076923076924\n"
     ]
    }
   ],
   "source": [
    "print(\"SVC: \",accuracy_score(test_prediction1, submit['label'].values))\n",
    "print(\"RNS: \",accuracy_score(test_prediction2Aux, submit['label'].values))\n",
    "print(\"RNC: \",accuracy_score(test_prediction3Aux, submit['label'].values))\n",
    "print(\"RNT: \",accuracy_score(test_prediction4Aux, submit['label'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Union de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este implementacion de unir el resultado de los 4 modelos se ha hecho con el objetivo de intentar solventar la carencias de cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionSuma = []\n",
    "for i in range(len(submit['label'])):\n",
    "    suma = test_prediction1[i] + test_prediction2Aux[i] + test_prediction3Aux[i] + test_prediction4Aux[i]\n",
    "    if(suma == 0 or suma == 1):\n",
    "        predictionSuma.append(0)\n",
    "    elif(suma == 3 or suma == 4):\n",
    "        predictionSuma.append(1)\n",
    "    else:\n",
    "        predictionSuma.append(test_prediction4Aux[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union de modelos:  0.7625\n"
     ]
    }
   ],
   "source": [
    "print(\"Union de modelos: \",accuracy_score(predictionSuma, submit['label'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha aumentado la precision pero de forma muy poco significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluación externa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte utilizamos un enlace de una noticia externa la dataset para tratar de obtener la información necesaria de la misma. Para después procesarla como hariamos con otra noticia cualquiera del dataset.\n",
    "\n",
    "El siguiente método, mediante la utilización de la libreria BeautifulSoup, es el que extrae los atributos necesarios del enlace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfo(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    titulo1 = soup.find('h1')\n",
    "    titulo2 = soup.find('title')\n",
    "    titulo = \"\"\n",
    "    if titulo1 != None:\n",
    "        titulo += titulo1.get_text()\n",
    "    elif titulo2 != None:\n",
    "        titulo += titulo2.get_text()\n",
    "    else: \n",
    "        titulo = \"\"\n",
    "\n",
    "    author1 = soup.find('span', class_='author')\n",
    "    author2 = soup.find('span', class_='byline__name')\n",
    "    author3 = soup.find('div', class_='author')\n",
    "    author4 = soup.find('div', class_='byline__name')\n",
    "    author_tag = soup.find('meta', attrs={'name': 'author'})\n",
    "    author5 = author_tag['content'] if author_tag else None\n",
    "\n",
    "    author = \"\"\n",
    "    if author1 != None:\n",
    "        author += author1.get_text()\n",
    "    elif author2 != None:\n",
    "        author += author2.get_text()\n",
    "    elif author3 != None:\n",
    "        author += author3.get_text()\n",
    "    elif author4 != None:\n",
    "        author += author4.get_text()\n",
    "    elif author5 != None:\n",
    "        author += author5    \n",
    "    else: \n",
    "        author = \"\"    \n",
    "        \n",
    "    return titulo + author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlace = \"https://www.breitbart.com/sports/2017/05/23/social-media-happy-shot-elephant-crushed-a-hunter-to-death/amp/\"\n",
    "noticiaExterna = getInfo(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticiaExterna = filtro(noticiaExterna)\n",
    "noticiaExterna = [noticiaExterna]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la información que queda después de procesarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['social media happi shot eleph crush hunter death']\n"
     ]
    }
   ],
   "source": [
    "print(noticiaExterna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticiaExternaVec = vectorizer.transform(noticiaExterna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 23135)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticiaExternaVec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducimos su dimensionalidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticiaExternaVec = tsvd.transform(noticiaExternaVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticiaExternaVec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente somos capaces de analizar la noticia como si fuese del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "predictionExterna1 = SVC.predict(noticiaExternaVec)\n",
    "predictionExterna2 = RNS.predict(noticiaExternaVec)\n",
    "predictionExterna3 = RNC.predict(noticiaExternaVec)\n",
    "predictionExterna4 = RNT.predict(noticiaExternaVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[0.6197667]]\n",
      "[[0.66725016]]\n",
      "[[0.61427575]]\n"
     ]
    }
   ],
   "source": [
    "print(predictionExterna1)\n",
    "print(predictionExterna2)\n",
    "print(predictionExterna3)\n",
    "print(predictionExterna4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exportar archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar en otro proyecto los elementos vectorizer, tsvd y los modelos entrenados sin tener que volver a procesarlos es necesario su exportación a un archivo. Nosotros utilizaremos la libreria pickle y el metodo save de keras tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tsvd.pkl', 'wb') as f:\n",
    "    pickle.dump(tsvd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svc.pkl', 'wb') as f:\n",
    "    pickle.dump(SVC, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNS.save('rns.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNC.save('rnc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNT.save('rnt.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
